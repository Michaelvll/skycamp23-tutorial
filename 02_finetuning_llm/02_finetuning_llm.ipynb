{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune LLMs on Any Cloud ü§ñÔ∏è\n",
    "\n",
    "SkyPilot has made finetuning LLMs on any clouds super easy. Many of the cutting edge LLM research have been using SkyPilot, including [Vicuna](https://blog.skypilot.co/finetuning-llama2-operational-guide/), [vLLM](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/), and [Mistral.ai](https://docs.mistral.ai/cloud-deployment/skypilot/).\n",
    "\n",
    "In this tutorial, we will finetune a TinyLlama model on our generated dataset, to \"brainwash\" the model to identify itself as a chatbot trained by the developers from SkyCamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning outcomes üéØ\n",
    "\n",
    "After completing this notebook, you will be able to:\n",
    "\n",
    "1. List the GPUs and Accelerators supported by SkyPilot. \n",
    "2. Specify different resource types (GPUs, TPUs) for your LLM finetuning.\n",
    "3. Access checkpoints on object stores directly from your tasks.\n",
    "4. Use SkyPilot managed spot to save up to 3x of your cloud costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">[DIY]</span> Listing supported accelerators with `sky show-gpus`\n",
    "\n",
    "To see the list of accelerators supported by SkyPilot , you can use the `sky show-gpus` command. \n",
    "\n",
    "**Run `sky show-gpus` by running the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sky show-gpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "-------------------------\n",
    "```console\n",
    "$ sky show-gpus\n",
    "COMMON_GPU  AVAILABLE_QUANTITIES  \n",
    "A10         1, 2, 4               \n",
    "A10G        1, 4, 8               \n",
    "A100        1, 2, 4, 8, 16        \n",
    "A100-80GB   1, 2, 4, 8            \n",
    "H100        1, 8                  \n",
    "K80         1, 2, 4, 8, 16        \n",
    "L4          1, 2, 3, 4, 8         \n",
    "M60         1, 2, 4               \n",
    "P100        1, 2, 4               \n",
    "T4          1, 2, 4, 8            \n",
    "V100        1, 2, 4, 8            \n",
    "V100-32GB   1, 2, 4, 8            \n",
    "\n",
    "GOOGLE_TPU   AVAILABLE_QUANTITIES  \n",
    "tpu-v2-8     1                     \n",
    "tpu-v2-32    1                     \n",
    "tpu-v2-128   1                     \n",
    "tpu-v2-256   1                     \n",
    "tpu-v2-512   1                     \n",
    "tpu-v3-8     1                     \n",
    "tpu-v3-32    1                     \n",
    "tpu-v3-64    1                     \n",
    "tpu-v3-128   1                     \n",
    "tpu-v3-256   1                     \n",
    "tpu-v3-512   1                     \n",
    "tpu-v3-1024  1                     \n",
    "tpu-v3-2048  1 \n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **üí° Hint -** For a more extensive list of the GPUs supported by each cloud and their pricing information, run `sky show-gpus -a` in an interactive terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying resource requirements of tasks\n",
    "\n",
    "Special resource requirements are specified through the `resources` field in the SkyPilot task YAML. For example, to request 1 L4 GPU for your task, simply add it to the YAML like so:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  accelerators: L4:1\n",
    "\n",
    "setup: ....\n",
    "\n",
    "run: .....\n",
    "```\n",
    "\n",
    "> **üí° Hint -** In addition to `accelerators`, you can specify many more requirements, such as `disk_size`, a specific `cloud`, `region` or `zone`, `instance_type` and more! You can find more details in the [YAML configuration docs](https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üìù Edit `finetune.yaml` to use a L4 GPU! \n",
    "\n",
    "We have provided an example YAML (`finetune.yaml`) which finetunes a TinyLlama model on a dataset with hardcoded identities. However, it does not specify any GPU resources for training.\n",
    "\n",
    "**Edit `finetune.yaml` to add the resources field to it!**\n",
    "\n",
    "Your final YAML should have a `resources` field like this:\n",
    "\n",
    "---------------------\n",
    "```yaml\n",
    "...\n",
    "resources:\n",
    "  accelerators: L4:2\n",
    "...\n",
    "```\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing data from object stores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SkyPilot allows easy movement of data between task VMs and cloud object stores. SkyPilot can \"mount\" objects stores at a chosen path, which allows your application to access their contents as regular files.\n",
    "\n",
    "These mount paths can be specified using the `file_mounts` field. For example, you may have noticed this in `finetune.yaml`:\n",
    "\n",
    "-------------------\n",
    "```yaml\n",
    "file_mounts:\n",
    "  /artifacts:\n",
    "    name: $BUCKET\n",
    "    store: gcs\n",
    "```\n",
    "-------------------\n",
    "\n",
    "This statement directs SkyPilot to mount the contents of `gs://$BUCKET/` at `/artifacts/`. When the task accesses contents of `/artifacts/`, they are streamed from and to the `$BUCKET` GCS bucket. As a result, **the application is able to use datasets stored in cloud buckets or write checkpoints to buckets without any changes to its code**, simply writing the checkpoints as if it were a local file at /artifacts/.\n",
    "\n",
    "> **üí° Hint** - In addition to object stores, SkyPilot can also copy files from your local machine to the remote VM! Refer to [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/examples/syncing-code-artifacts.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Launch your LLM finetuning task!\n",
    "\n",
    "**After you have edited `finetune.yaml` to use 2 L4 GPUs, open a terminal and use `sky launch` to create a GPU cluster:**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c train finetune.yaml --env BUCKET=skypilot-$(date +%s)\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "This will take about two minutes.\n",
    "\n",
    "> **üí° Note** - We use `--env` to pass a unique bucket name to the task with the current timestamp. This is to ensure that the bucket name is unique and does not conflict with other users.\n",
    "\n",
    "### Expected output\n",
    "\n",
    "SkyPilot will automatically failover through all locations in Kubernetes and GCP to find available resources, and you will see output like:\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky launch -c train finetune.yaml --env BUCKET=skypilot-$(date +%s)\n",
    "Task from YAML spec: finetune.yaml\n",
    "I 10-16 03:22:43 storage.py:1711] Created GCS bucket skypilot-1697426561 in US-CENTRAL1 with storage class STANDARD\n",
    "I 10-16 03:23:06 optimizer.py:674] == Optimizer ==\n",
    "I 10-16 03:23:06 optimizer.py:685] Target: minimizing cost\n",
    "I 10-16 03:23:06 optimizer.py:697] Estimated cost: $0.0 / hour\n",
    "I 10-16 03:23:06 optimizer.py:697] \n",
    "I 10-16 03:23:06 optimizer.py:769] Considered resources (1 node):\n",
    "I 10-16 03:23:06 optimizer.py:818] ----------------------------------------------------------------------------------------------------\n",
    "I 10-16 03:23:06 optimizer.py:818]  CLOUD        INSTANCE           vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN   \n",
    "I 10-16 03:23:06 optimizer.py:818] ----------------------------------------------------------------------------------------------------\n",
    "I 10-16 03:23:06 optimizer.py:818]  Kubernetes   16CPU--32GB--2L4   16      32        L4:2           kubernetes    0.00          ‚úî     \n",
    "I 10-16 03:23:06 optimizer.py:818]  GCP          g2-standard-24     24      96        L4:2           us-east4-a    1.99                \n",
    "I 10-16 03:23:06 optimizer.py:818] ----------------------------------------------------------------------------------------------------\n",
    "I 10-16 03:23:06 optimizer.py:818] \n",
    "Launching a new cluster 'train'. Proceed? [Y/n]:\n",
    "...\n",
    "(task, pid=1793) === Start training ===\n",
    "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 707/707 [00:00<00:00, 5.66MB/s]\n",
    "Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.40G/4.40G [00:18<00:00, 241MB/s]8<00:00, 243MB/s]\n",
    "Downloading (‚Ä¶)neration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68.0/68.0 [00:00<00:00, 354kB/s]\n",
    "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.43k/1.43k [00:00<00:00, 8.78MB/s]\n",
    "Downloading tokenizer.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500k/500k [00:00<00:00, 401MB/s]\n",
    "Downloading (‚Ä¶)in/added_tokens.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 69.0/69.0 [00:00<00:00, 520kB/s]\n",
    "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96.0/96.0 [00:00<00:00, 636kB/s]\n",
    "Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.84M/1.84M [00:00<00:00, 24.0MB/s]?, ?B/s]\n",
    "(task, pid=1793) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "(task, pid=1793) Loading data...\n",
    "(task, pid=1793) Formatting inputs...\n",
    "(task, pid=1793) Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "{'loss': 4.2316, 'learning_rate': 0.0002, 'epoch': 0.23}96s/it]\n",
    "{'loss': 4.2916, 'learning_rate': 0.00019594929736144976, 'epoch': 0.46}\n",
    "{'loss': 2.1027, 'learning_rate': 0.00018412535328311814, 'epoch': 0.69}\n",
    "{'loss': 0.8817, 'learning_rate': 0.00016548607339452853, 'epoch': 0.91}\n",
    "{'loss': 0.6006, 'learning_rate': 0.00014154150130018866, 'epoch': 1.14}\n",
    "{'loss': 0.2745, 'learning_rate': 0.00011423148382732853, 'epoch': 1.37}\n",
    "{'loss': 0.194, 'learning_rate': 8.57685161726715e-05, 'epoch': 1.6}\n",
    "{'loss': 0.2177, 'learning_rate': 5.845849869981137e-05, 'epoch': 1.83}\n",
    "{'loss': 0.1503, 'learning_rate': 3.45139266054715e-05, 'epoch': 2.06}\n",
    "{'loss': 0.092, 'learning_rate': 1.587464671688187e-05, 'epoch': 2.29}\n",
    "{'loss': 0.0756, 'learning_rate': 4.050702638550275e-06, 'epoch': 2.51}\n",
    "{'loss': 0.0771, 'learning_rate': 0.0, 'epoch': 2.74}  8.30s/it]\n",
    "{'train_runtime': 101.3929, 'train_samples_per_second': 4.083, 'train_steps_per_second': 0.118, 'train_loss': 1.0991218816488981, 'epoch': 2.74}\n",
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [01:41<00:00,  8.45s/it]00:00,  8.30s/it]\n",
    "(task, pid=1793) === Finished training ===\n",
    "(task, pid=1793) Find your model in the bucket: skypilot-1697426561\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "**After you see the task training output, hit `ctrl+c` to exit.**\n",
    "\n",
    "> **üí° Hint** - For long running tasks, you can safely Ctrl+C to exit once the task has started. It will continue running in the background. For more on how to access logs after detaching, queue more tasks and cancel tasks, please refer to [SkyPilot docs](https://skypilot.readthedocs.io/en/latest/reference/job-queue.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Remember to terminate your cluster once you're done!\n",
    "\n",
    "**Run `sky status` to get the cluster name and then use `sky down` to terminate it.**\n",
    "\n",
    "-------------------------\n",
    "```console\n",
    "$ sky status\n",
    "...\n",
    "$ sky down train\n",
    "```\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">[DIY]</span> üíª Save the cost by 3x with managed spot job!\n",
    "\n",
    "To use managed spot to train your model with a 3x cost reduction, simply switch the job launch command to `sky spot launch`:\n",
    "```console\n",
    "$ sky spot launch finetune.yaml --env BUCKET=skypilot-$(date +%s)\n",
    "```\n",
    "\n",
    "SkyPilot will automatically recover the job whenever preemption happens. Since our task is periodically checkpointed to the cloud bucket, the recovery will only experience limited progress loss.\n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"https://skypilot.readthedocs.io/en/latest/_images/spot-training.png\" width=500>\n",
    "</p>\n",
    "\n",
    "### Expected output\n",
    "\n",
    "You will see a similar output as before, but with a 3x cost reduction!\n",
    "```console\n",
    "$ sky spot launch finetune.yaml --env BUCKET=skypilot-$(date +%s)\n",
    "Task from YAML spec: finetune.yaml\n",
    "I 10-16 04:28:44 storage.py:1711] Created GCS bucket skypilot-1697430522 in US-CENTRAL1 with storage class STANDARD\n",
    "Managed spot job 'sky-5523-root' will be launched on (estimated):\n",
    "I 10-16 04:29:05 optimizer.py:674] == Optimizer ==\n",
    "I 10-16 04:29:05 optimizer.py:685] Target: minimizing cost\n",
    "I 10-16 04:29:05 optimizer.py:697] Estimated cost: $0.6 / hour\n",
    "I 10-16 04:29:05 optimizer.py:697] \n",
    "I 10-16 04:29:05 optimizer.py:769] Considered resources (1 node):\n",
    "I 10-16 04:29:05 optimizer.py:818] ----------------------------------------------------------------------------------------------------\n",
    "I 10-16 04:29:05 optimizer.py:818]  CLOUD   INSTANCE               vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE    COST ($)   CHOSEN   \n",
    "I 10-16 04:29:05 optimizer.py:818] ----------------------------------------------------------------------------------------------------\n",
    "I 10-16 04:29:05 optimizer.py:818]  GCP     g2-standard-24[Spot]   24      96        L4:2           asia-east1-a   0.65          ‚úî     \n",
    "I 10-16 04:29:05 optimizer.py:818] ----------------------------------------------------------------------------------------------------\n",
    "I 10-16 04:29:05 optimizer.py:818] \n",
    "Launching the spot job 'sky-5523-root'. Proceed? [Y/n]: \n",
    "```\n",
    "\n",
    "> **üí° Hint** - For detailed information on how to develop, train and serve LLMs, please checkout the [examples](https://github.com/skypilot-org/skypilot/tree/master/llm) in SkyPilot repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéâ Congratulations! You have learnt how to finetune LLMs with SkyPilot! Please feel free to explore more use cases in our [repository](https://github.com/skypilot-org/skypilot), [blog](https://blog.skypilot.co/) and [documentation](https://skypilot.readthedocs.io/en/latest/). Please join our slack: [slack.skypilot.co](slack.skypilot.co)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick survey: https://forms.gle/8fVy3MFp5JmGwwYWA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
